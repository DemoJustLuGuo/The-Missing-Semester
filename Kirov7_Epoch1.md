# CFC Studio 共学 Epoch1 指引

---

# Kirov7

> kirov reporting

## 笔记证明

<!-- Content_START --> 
### 1.11
#### LangChain 框架

LangChain 主要提供了 6 大核心组件帮助我们更好的使用大语言模型，涵盖了 Models（模型）、Prompts（提示）、Indexes（索引）、Memory（记忆）、Chains（链）、Agents（代理），这些组件集成了数十种大语言模型、多样的知识库处理方法以及成熟的应用链、上百种可调用的工具箱，为用户提供了一个快速搭建和部署大语言模型智能应用程序的平台。
![LangChain](https://heap.crazyfay.com/uploads/1736601823.png)

#### Prompt 组件

大多数 LLM 应用程序都不会直接将用户输入传递给 LLM。通常，它们会将用户输入添加到一个更大的文本片段中，称为提示模板，该模板提供有关特定任务的附加上下文。

大多数 LLM 应用程序都不会直接将用户输入传递给 LLM。通常，它们会将用户输入添加到一个更大的文本片段中，称为提示模板，该模板提供有关特定任务的附加上下文。并且 Prompt 是所有 AI 应用交互的起点，以下是 LangChain 中一个最基础的聊天应用机器人的运行流程如下：
![LangChain](https://heap.crazyfay.com/uploads/1736601994.jpg)

为了适配不同的 LLM，LangChain 封装了 Prompt 组件，并且 Prompt 组件是高可移植性的，同一个 Prompt 可以支持各种 LLM，在切换 LLM 的时候，无需修改 Prompt。在 LangChain 中，Prompt 被分成了两大类：

- **Prompt Template**：将 Prompt 按照 template 进行一定格式化，针对 Prompt 进行变量处理以及提示词的组合。
- **Selectors**：根据不同条件去选择不同提示词，或者在不同情况下通过 Selector，选择不同示例去进一步提高 Prompt 支持能力。
    >  本质上 Selectors 只是 Prompt Template 的二次封装

对于 Prompt Template，在 LangChain 中，又涵盖了多个子组件，例如：角色提示模板、消息占位符、文本提示模板、聊天消息提示模板、提示、消息等，Prompt Template 的运行流程如下：


![Prompt](https://heap.crazyfay.com/uploads/1736601929.jpg)

不同 Prompt 组件功能的简介：

- **PromptTemplate**：用于创建文本消息提示模板，用于用于与大语言模型/文本生成模型进行交互。
- **ChatPromptTemplate**：用于创建聊天消息提示模板，一般用于与聊天模型进行交互。
- **MessagePlaceholder**：消息占位符，在聊天模型中对不确定是否需要的消息进行占位。
- **SystemMessagePromptTemplate**：用于创建系统消息提示模板，角色为系统。
- **HumanMessagePromptTemplate**：用于创建人类消息提示模板，角色为人类。
- **AIMessagePromptTemplate**：用于创建AI消息提示模板，角色为AI。
- **PipelinePromptTemplate**：用于创建管道消息，管道消息可以将提示模板作为变量进行快速复用。

Prompt 不同方法的功能简介：

- **partial**：用于格式化提示模板中的部分变量。
- **format**：传递变量数据，格式化提示模板为文本消息。
- **invoke**：传递变量数据，格式化提示模板为提示。
- **to_string**：将提示/消息提示列表转换成字符串。
- **to_messages**：用于将提示转换成消息列表。

Prompt 中重载的运算符：

- **+ 运算符**：在 Prompt 组件中，对 + 运算符使用 `__add__` 方法进行重写，所以几乎所有 Prompt 组件都可以使用 + 进行组装拼接。

#### Model 组件
Model 是 LangChain 的核心组件，但是 LangChain 本身不提供自己的 LLM，而是提供了一个标准接口，用于封装不同类型的 LLM 进行交互，其中 LangChain 为两种类型的模型提供接口和集成：

- **LLM**：使用纯文本作为输入和输出的大语言模型。
- **Chat Model**：使用聊天消息列表作为输入并返回聊天消息的聊天模型。

在 LangChain 中，无论是 LLM 亦或者 Chat Model 都可以接受 PromptValue/字符串/消息列表 作为参数，内部会根据模型的类型自动转换成字符串亦或者消息列表，屏蔽了不同模型的差异。

对于 Model 组件，LangChain 有一个模型总基类，并对基类进行了划分：
![Model](https://heap.crazyfay.com/uploads/1736602318.jpg)

调用大模型最常用的方法为：

- **invoke**：传递对应的文本提示/消息提示，大语言模型生成对应的内容。
- **batch**：invoke 的批量版本，可以一次性生成多个内容。
- **stream**：invoke 的流式输出版本，大语言模型每生成一个字符就返回一个字符。

基础聊天应用的运行流程更改成如下：
![Model](https://heap.crazyfay.com/uploads/1736602471.jpg)

### 1.12

#### LLM 实现记忆功能思路
大多数的 LLM 应用程序都会有一个会话接口，允许我们和 LLM 进行多轮对话，并有一定的上下文记忆功能。但实际上，模型本身时不会记忆任何上下文的，只能依靠用户本身的输入去产生输出。而实现这个记忆功能，就需要额外的模块去保存我们和模型对话的上下文信息，然后在下一次请求时，把所有的历史信息都输入给模型，让模型输出结果。

所以为 LLM 添加记忆其实非常简单，就是在 Prompt 中预留 `chat_history` 占位符，将 `Human/Ai` 的历史对话信息插入到占位符中，并且实时保存 `Human/Ai` 的对话信息，在每一次对话时插入到预留占位符即可完成最简单的记忆功能。


#### 常见记忆模式
基于在 Prompt 中插入记忆内容，可以划分成几种记忆模式，例如：缓冲记忆、缓冲窗口记忆、令牌缓冲记忆、摘要总结记忆、摘要缓冲混合记忆、实体记忆、向量存储库记忆等，不同的记忆模式有不同的适用场景。

##### 缓冲记忆
最基础的记忆模式，将所有 `Human/Ai` 生成的消息全部存储起来，每次需要使用时将保存的所有聊天消息列表传递到 Prompt 中，通过往用户的输入中添加历史对话信息/记忆，可以让 LLM 能理解之前的对话内容，而且这种记忆方式在上下文窗口限制内是无损的。
- **优点**：
    1. 无损记忆，用户输入什么内容都会被记忆；
    2. 实现方式简单，兼容性最好，所有大模型都支持。
- **缺点**：
    1. 直接将存储的所有内容给 LLM，因为大量信息意味着新输入中包含更多的 Token，导致响应时间变慢和成本增加。
    2. 当达到 LLM 的令牌数限制时，太长的对话无法被记住。
    3. 记忆内容不是无限的，对于上下文长度较小的模型来说，记忆内容会变得极短。


##### 缓冲窗口记忆
缓冲窗口记忆只保存最近的几次 `Human/Ai` 生成的消息，它基于 `缓冲记忆` 思想，并添加了一个窗口值 `k`，这意味着只保留一定数量的过去互动，然后“忘记”之前的互动。
- **优点**：
    1. 缓冲窗口记忆在限制使用的 Token 数量表现优异。
    2. 对小模型也比较友好，不提问比较远的关联内容，一般效果最佳。
    3. 实现方式简单，性能优异，所有大模型都支持。
- **缺点**：
    1. 缓冲窗口记忆不适合遥远的互动，会忘记之前的“互动”。
    2. 部分对话内容长度较大，容易超过 LLM 的上下文限制。

##### 令牌缓冲记忆
缓冲窗口记忆只保存限定次数 `Human/Ai` 生成的消息，它基于 `缓冲记忆` 思想，并添加了一个令牌数 `max_tokens`，当聊天历史超过令牌数时，会遗忘之前的互动。

- **优点**：
    1. 可以基于大语言模型的上下文长度限制分配记忆长度。
    2. 对小模型也比较友好，不提问比较远的关联内容，一般效果最佳。
    3. 实现方式简单，性能优异，所有大模型都支持。
- **缺点**：
    1. 令牌缓冲记忆不适合遥远的互动，会忘记之前的“互动”。

##### 摘要总结记忆
除了将消息传递给 LLM，还可以将消息进行总结，每次只传递总结的信息，而不是完整的消息。这种模式记忆对于较长的对话最有用，可以避免过度使用 Token，因为将过去的信息历史以原文的形式保留在提示中会占用太多的 Token。

- **优点**：
    1. 无论是长期还是短期的互动都可以记忆（模糊记忆）。
    2. 减少长对话中使用 Token 的数量，能记忆更多轮的对话信息。
    3. 长对话时效果明显，虽然最初使用 Token 数量较多，随着对话进行，摘要方法增长速度减慢，与常规缓冲内存模型相比具有优势。
- **缺点**：
    1. 虽然能同时记住近期和长远的互动内容，但是记忆的细节部分会丢失；
    2. 对于较短的对话可能会增加 Token 使用量。
    3. 对话历史的记忆完全依赖于中间摘要 LLM 的能力，需要为摘要 LLM 分配 Token，增加成本且未限制对话长度。

##### 摘要缓冲混合记忆
摘要缓冲混合记忆结合了 `摘要总结记忆` 与 `缓冲窗口记忆`，它旨在对对话进行摘要总结，同时保留最近互动中的原始内容，但不是简单地清除旧的交互，而是将它们编译成摘要并同时使用，并且使用标记长度而不是交互数量来确定何时清除交互。

- **优点**：
    1. 无论是长期还是短期的互动都可以记忆，长期为模糊记忆，短期为精准记忆。
    2. 减少长对话中使用 Token 的数量，能记忆更多轮的对话信息。
- **缺点**：
    1. 长期互动的内容仍然为模糊记忆。
    2. 总结摘要部分完全依赖于中间摘要 LLM 的能力，需要为摘要 LLM 分配 Token，增加成本且未限制对话长度。

##### 向量存储库记忆
将记忆存储在向量存储中，并在每次调用时查询前 K 个最匹配的文档。这类记忆模式能记住所有内容，在细节部分比摘要总结要强，但是比缓冲记忆弱，消耗 Token 方面相对平衡。

- **优点**：
    1. 拥有比摘要总结更强的细节，比缓冲记忆能记忆更多的内容，甚至无限长度的内容；
    2. 消耗的 Token 也相对平衡；
- **缺点**：
    1. 性能相比其他模式相对较差，需要额外的 Embedding + 向量数据库支持。
    2. 记忆效果受检索功能的影响，好的非常好，差的非常差。

<!-- Content_END -->